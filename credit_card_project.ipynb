{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1195bf6c",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection and Analysis\n",
    "This notebook performs data preprocessing, classification, and clustering on a credit card fraud detection dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f063ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, auc, ConfusionMatrixDisplay\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba5c961",
   "metadata": {},
   "source": [
    "## 1. Define Utility Functions\n",
    "Functions for searching files, handling data, and running models are defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619e2dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to search for a CSV file by name\n",
    "def find_csv_file(filename, start_dir=\".\"):\n",
    "    for root, dirs, files in os.walk(start_dir):\n",
    "        if filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            print(f\"File found: {file_path}\")\n",
    "            return pd.read_csv(file_path)\n",
    "    print(f\"File '{filename}' not found.\")\n",
    "    return None\n",
    "\n",
    "# Function to preprocess data\n",
    "def preprocess_data(data, target_column):\n",
    "    # Check if target column exists\n",
    "    if target_column not in data.columns:\n",
    "        raise ValueError(f\"Target column '{target_column}' not found in dataset.\")\n",
    "\n",
    "    # Drop rows where the target column has NaN\n",
    "    print(\"Handling missing values in the target column...\")\n",
    "    data = data.dropna(subset=[target_column])\n",
    "\n",
    "    # Separate features and target\n",
    "    X = data.drop(columns=[target_column])\n",
    "    y = data[target_column]\n",
    "\n",
    "    # Handle missing values in features\n",
    "    print(\"Handling missing values in features...\")\n",
    "    X = X.fillna(X.median(numeric_only=True))  # Fill numeric NaN with median\n",
    "    X = X.fillna(\"Missing\")  # Fill categorical NaN with a placeholder value\n",
    "\n",
    "    # Identify numeric and categorical columns\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    # Debugging: Print categorical columns before processing\n",
    "    print(f\"Categorical columns before processing: {categorical_cols}\")\n",
    "\n",
    "    # Handle categorical variables with high cardinality\n",
    "    high_cardinality_cols = [col for col in categorical_cols if X[col].nunique() > 100]\n",
    "    if high_cardinality_cols:\n",
    "        print(f\"Dropping high-cardinality columns: {high_cardinality_cols}\")\n",
    "        X = X.drop(columns=high_cardinality_cols)\n",
    "\n",
    "    # Update the list of categorical columns after dropping high-cardinality ones\n",
    "    categorical_cols = [col for col in categorical_cols if col not in high_cardinality_cols]\n",
    "\n",
    "    # Apply one-hot encoding to remaining categorical variables\n",
    "    X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85273a32",
   "metadata": {},
   "source": [
    "## 2. Define Model Functions\n",
    "This section includes functions to train models and evaluate their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33c2d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate a model\n",
    "def train_and_evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"{model_name} Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09361274",
   "metadata": {},
   "source": [
    "## 3. Main Workflow\n",
    "This section trains all the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab4198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X_train, X_test, y_train, y_test):\n",
    "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Classification Report\n",
    "    print(\"Logistic Regression Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, cmap='Blues')\n",
    "    plt.title(\"Logistic Regression - Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    # ROC Curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve (area = {roc_auc:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # Random classifier line\n",
    "    plt.title(\"Logistic Regression - ROC Curve\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Decision Tree Classifier\n",
    "def decision_tree_classifier(X_train, X_test, y_train, y_test):\n",
    "    model = DecisionTreeClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Classification Report\n",
    "    print(\"Decision Tree Classifier Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, cmap='Purples')\n",
    "    plt.title(\"Decision Tree - Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Random Forest Classifier\n",
    "def random_forest(X_train, X_test, y_train, y_test):\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Classification Report\n",
    "    print(\"Random Forest Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, cmap='Greens')\n",
    "    plt.title(\"Random Forest - Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    # ROC Curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve (area = {roc_auc:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # Random classifier line\n",
    "    plt.title(\"Random Forest - ROC Curve\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# K-Means Clustering\n",
    "def kmeans_clustering(X, n_clusters=2):\n",
    "    if X.shape[1] < 2:\n",
    "        raise ValueError(\"Clustering requires at least two features.\")\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "\n",
    "    # Visualizing the Clusters\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.5)\n",
    "    plt.title(f'K-Means Clustering (n_clusters={n_clusters})')\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.show()\n",
    "\n",
    "    return kmeans\n",
    "\n",
    "\n",
    "# DBSCAN Clustering\n",
    "def dbscan_clustering(X, eps=0.5, min_samples=5):\n",
    "    if X.shape[1] < 2:\n",
    "        raise ValueError(\"Clustering requires at least two features.\")\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(X)\n",
    "\n",
    "    # Visualizing the Clusters\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.5)\n",
    "    plt.title(f'DBSCAN Clustering (eps={eps}, min_samples={min_samples})')\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.show()\n",
    "\n",
    "    return dbscan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50529d87",
   "metadata": {},
   "source": [
    "## 4. Main Workflow\n",
    "This section loads the dataset, preprocesses it, and runs the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c95169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # File name of the merged dataset\n",
    "    filename = 'Combined_data_3920.csv'\n",
    "    \n",
    "    try:\n",
    "        # Search for the file on the desktop dynamically\n",
    "        print(\"Searching for the merged dataset...\")\n",
    "        merged = find_csv_file(filename, start_dir=os.path.join(os.path.expanduser(\"~\"), \"Desktop\"))\n",
    "        if merged is None:\n",
    "            print(f\"File '{filename}' not found on the Desktop.\")\n",
    "            return\n",
    "        print(\"Dataset loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load the dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    # Define target column\n",
    "    target_column = 'IsFraud'  # Update this to match your dataset's target column name\n",
    "    \n",
    "    if target_column not in merged.columns:\n",
    "        print(f\"Target column '{target_column}' not found in the dataset.\")\n",
    "        return\n",
    "\n",
    "    # Preprocess data\n",
    "    print(\"Preprocessing data...\")\n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = preprocess_data(merged, target_column)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during preprocessing: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Logistic Regression\n",
    "    print(\"Running Logistic Regression...\")\n",
    "    logistic_model = logistic_regression(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # Random Forest\n",
    "    print(\"\\nRunning Random Forest...\")\n",
    "    random_forest_model = random_forest(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # Decision Tree Classifier\n",
    "    print(\"\\nRunning Decision Tree Classifier...\")\n",
    "    decision_tree_model = decision_tree_classifier(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # Clustering\n",
    "    print(\"\\nScaling features for clustering...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    print(\"\\nRunning K-Means Clustering...\")\n",
    "    clustering_features = X_train_scaled[:, :2]  # Use only the first two scaled features\n",
    "    kmeans_model = kmeans_clustering(clustering_features)\n",
    "\n",
    "    print(\"\\nRunning DBSCAN Clustering...\")\n",
    "    dbscan_model = dbscan_clustering(clustering_features)\n",
    "\n",
    "    print(\"\\nAnalysis complete.\")\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
